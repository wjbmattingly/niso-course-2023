{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 7/7 [00:02<00:00,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete and data saved to 'processed_texts.parquet'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize the sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to load data\n",
    "def load_data(categories):\n",
    "    newsgroups_data = fetch_20newsgroups(subset='all', categories=categories,\n",
    "                                         remove=('headers', 'footers', 'quotes'))\n",
    "    return newsgroups_data.data[:200]  # Adjust the slice as needed\n",
    "\n",
    "# Function to preprocess documents\n",
    "def preprocess_documents(documents):\n",
    "    processed_data = {\n",
    "        'original_text': [],\n",
    "        'tokenized_text': [],\n",
    "        'hover_text': [],\n",
    "        'text_no_punct_lemma': [],\n",
    "        'text_no_punct': [],\n",
    "        'ml_vectors': []  # Placeholder for machine learning model vectors\n",
    "    }\n",
    "    \n",
    "    for doc in nlp.pipe(documents, disable=[\"ner\", \"parser\"]):  # Disable unnecessary pipelines\n",
    "        # Tokenized text\n",
    "\n",
    "        tokens = [token.text for token in doc]\n",
    "        processed_data['tokenized_text'].append(tokens)\n",
    "\n",
    "        # Hover text\n",
    "        hover_text = \"<br>\".join(tokens[:10]) + '...' if len(tokens) > 10 else \" \".join(tokens)\n",
    "        processed_data['hover_text'].append(hover_text)\n",
    "\n",
    "        # Text with no punctuation and lemmatization\n",
    "        text_no_punct_lemma = \" \".join(token.lemma_ for token in doc if not token.is_punct and token.lemma_ != '-PRON-')\n",
    "        processed_data['text_no_punct_lemma'].append(text_no_punct_lemma)\n",
    "\n",
    "        # Text with no punctuation\n",
    "        text_no_punct = \" \".join(token.text for token in doc if not token.is_punct)\n",
    "        processed_data['text_no_punct'].append(text_no_punct)\n",
    "\n",
    "        # Keep original text for reference\n",
    "        processed_data['original_text'].append(doc.text)\n",
    "    \n",
    "    # Generate machine learning model vectors for the lemmatized, no punctuation texts\n",
    "    ml_vectors = model.encode(processed_data['original_text'], show_progress_bar=True)\n",
    "    processed_data['ml_vectors'] = ml_vectors.tolist()\n",
    "\n",
    "    # Convert processed data to DataFrame\n",
    "    df = pd.DataFrame(processed_data)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Define your categories\n",
    "categories = [\"rec.sport.baseball\", \"sci.space\"]\n",
    "documents = load_data(categories)\n",
    "documents = [doc.replace(\"\\n\", \" \") for doc in documents]\n",
    "df_processed = preprocess_documents(documents)\n",
    "\n",
    "# Save to a parquet file\n",
    "df_processed.to_parquet('../data/processed_texts.parquet')\n",
    "\n",
    "print(\"Preprocessing complete and data saved to 'processed_texts.parquet'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Do you really have *that* much information on him?  Really?   I don't know.  You tell me.  What percentage of players reach or  exceed their MLE's *in their rookie season*?  We're talking about 1993, you know.   If that were your purpose, maybe.  Offerman spent 1992 getting  acclimated, if you will.  The Dodgers as a team paid a big price that season.  Perhaps they will reap the benefits down the road. Do you really think they would have done what they did if they were competing for a pennant?   For a stat-head, I'm amazed that you put any credence in spring training.  Did you notice who he got those 10 (!) hits off of, or are you going to tell me that it doesn't make a difference?   Wait a minute.  I missed something here.  First, forget Keith Mitchell.  Are you saying that a kid who moves from AA to AAA and then does not improve would have been better off making a direct leap to the majors?  If a player does well at AA and then does not improve at AAA, isn't that a sign that maybe he doesn't belong in the bigs?  Now, Keith Mitchell.  As I recall (no stat books handy - surprise!) he jumped from AA to Atlanta in 1991.  He did so well that he was returned to the minors, where he didn't do very well at all.  Now his career is in jeopardy.  So how does he fit in with your  point.  Good MLE's in AA.  Moved him right to the big club.  Now he's one step away from being traded or moved out of baseball. Duh.    Well, I've cast my lot.  Certainly you may understand better how  good Lopez is.  And I may overvalue experience.  But neither one of us runs a baseball team.    --\\tThe Beastmaster  \""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "niso",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
